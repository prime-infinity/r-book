---
title: "case_study"
runtime: shiny
output: html_document
date: "2024-02-11"
params:
  selected_dataset: 5
  selected_query_type: 2
  query_count_table: NA
  
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
```

## Evaluating Model performance
Because each model has unique inputs and outputs, we can evaluate each model on some relevant performance metric. Our main aim is to evaluate how effective each model is at returning the highest number of "potentially relevant results that are subsequently confirmed to be truly relevant.

There are several metrics that we could use to evaluate the performance of each iteration of the analysis pipeline, each of which is uniquely defined by 1) the query generation method, 2) our choice of topic model, and 3) the specification of the relevance function.

We considered several options, which are listed below.

|  	| **Evaluation metric** 	| **Definition** 	| **Benefits** 	| **Other considerations** 	|
|---	|:---:	|:---:	|:---:	|:---:	|
| 1 	| Precision 	| xx 	| xx 	| xx 	|
| 2 	| Recall 	|  xx 	| xx 	| xx 	|
| 3 	| Percent unique 	| xx 	| xx 	| xx 	|


Our primary metric will be XXX based on the work of YYY.